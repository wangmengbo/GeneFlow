import os
import sys
import torch
import logging
from tqdm import tqdm
from torchmetrics.aggregation import MeanMetric # Assuming this is used, if not, can be removed
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from rectified.rectified_flow import DOPRI5Solver # Keep existing imports

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

class NativeScalerWithGradNormCount:
    """Gradient scaling utility for efficient mixed precision training."""
    def __init__(self, enabled=True): # Added enabled flag
        self._scaler = torch.cuda.amp.GradScaler(enabled=enabled)
        self.loss_scale = 2**16 # Default, can be adjusted by scaler
        self.inv_scale = 1. / self.loss_scale # Will be updated if scaler changes scale
        self.grad_norm = 0
        self.enabled = enabled

    def __call__(self, loss, optimizer, parameters, update_grad=True):
        if not self.enabled:
            loss.backward()
            if update_grad:
                # Optionally clip gradients even without scaler
                # self.grad_norm = torch.nn.utils.clip_grad_norm_(parameters, 1.0)
                optimizer.step()
                optimizer.zero_grad()
            return

        # For AMP:
        # loss = loss * self.loss_scale # Scaler handles this internally
        self._scaler.scale(loss).backward()

        if update_grad:
            # Unscale gradients before clipping
            self._scaler.unscale_(optimizer)
            # Clip gradients
            self.grad_norm = torch.nn.utils.clip_grad_norm_(parameters, 1.0) # Typical max_norm is 1.0
            # Optimizer step
            self._scaler.step(optimizer)
            # Update scaler
            self._scaler.update()
            optimizer.zero_grad()
            # Update inv_scale if necessary
            current_scale = self._scaler.get_scale()
            if current_scale != 0: # Avoid division by zero if scale becomes 0
                 self.loss_scale = current_scale
                 self.inv_scale = 1. / current_scale


    def state_dict(self):
        return self._scaler.state_dict()

    def load_state_dict(self, state_dict):
        self._scaler.load_state_dict(state_dict)

def train_with_rectified_flow(
    model,
    train_loader,
    val_loader,
    rectified_flow,
    device,
    num_epochs, # Total number of epochs to train for
    lr,
    best_model_path,
    latest_checkpoint_path, # Path for saving/loading the latest resumable checkpoint
    patience,
    use_amp,
    weight_decay,
    is_multi_cell,
    # New parameters for resuming
    start_epoch=0,
    initial_train_losses=None,
    initial_val_losses=None,
    initial_best_val_loss=float('inf'),
    initial_epochs_no_improve=0,
    optimizer_state_dict=None,
    scheduler_state_dict=None,
    scaler_state_dict=None
):
    """Train the RNA to H&E cell image generator model with rectified flow, supporting preemption."""
    model.to(device)

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
        betas=(0.9, 0.95),
        weight_decay=weight_decay
    )
    if optimizer_state_dict:
        optimizer.load_state_dict(optimizer_state_dict)
        logger.info("Loaded optimizer state from checkpoint.")

    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer=optimizer,
        T_max=num_epochs, # Total epochs
        eta_min=lr * 0.01
    )
    if scheduler_state_dict:
        lr_scheduler.load_state_dict(scheduler_state_dict)
        logger.info("Loaded learning rate scheduler state from checkpoint.")
    elif start_epoch > 0: # Adjust scheduler if resuming without saved state but from a later epoch
        for _ in range(start_epoch):
            lr_scheduler.step()
        logger.info(f"Advanced LR scheduler to epoch {start_epoch}.")


    loss_scaler = NativeScalerWithGradNormCount(enabled=use_amp)
    if use_amp and scaler_state_dict:
        loss_scaler.load_state_dict(scaler_state_dict)
        logger.info("Loaded AMP scaler state from checkpoint.")

    train_loss_metric = MeanMetric().to(device)
    val_loss_metric = MeanMetric().to(device)

    train_losses = initial_train_losses if initial_train_losses is not None else []
    val_losses = initial_val_losses if initial_val_losses is not None else []
    best_val_loss = initial_best_val_loss
    epochs_no_improve = initial_epochs_no_improve

    logger.info(f"Starting training from epoch {start_epoch} up to {num_epochs}.")

    for epoch in range(start_epoch, num_epochs):
        model.train()
        train_loss_metric.reset()
        
        # Use tqdm for progress bar
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Training", leave=False)
        for batch in progress_bar:
            gene_expr = batch['gene_expr'].to(device)
            target_images = batch['image'].to(device)
            gene_mask = batch.get('gene_mask', None)
            if gene_mask is not None:
                gene_mask = gene_mask.to(device)
            num_cells = None
            if is_multi_cell and 'num_cells' in batch:
                num_cells = batch['num_cells'] # This should be a tensor of cell counts

            t = torch.rand(gene_expr.shape[0], device=device)
            path_sample = rectified_flow.sample_path(x_1=target_images, t=t)
            x_t = path_sample["x_t"]
            target_velocity = path_sample["velocity"]

            # Common handling for L1 penalty
            l1_lambda = 0.001 # Define L1 lambda

            with torch.amp.autocast(device_type=device.type, enabled=use_amp):
                if is_multi_cell:
                    v_pred = model(x_t, t, gene_expr, num_cells, gene_mask)
                    # Assuming rna_encoder.cell_encoder is a list/Sequential and first element has weights
                    if hasattr(model, 'rna_encoder') and hasattr(model.rna_encoder, 'cell_encoder') and \
                       isinstance(model.rna_encoder.cell_encoder, (nn.Sequential, nn.ModuleList)) and \
                       len(model.rna_encoder.cell_encoder) > 0 and hasattr(model.rna_encoder.cell_encoder[0], 'weight'):
                        l1_penalty = torch.sum(torch.abs(model.rna_encoder.cell_encoder[0].weight)) * l1_lambda
                    else:
                        l1_penalty = torch.tensor(0.0, device=device) # No L1 if structure differs
                else:
                    v_pred = model(x_t, t, gene_expr, gene_mask)
                    if hasattr(model, 'rna_encoder') and hasattr(model.rna_encoder, 'encoder') and \
                       isinstance(model.rna_encoder.encoder, (nn.Sequential, nn.ModuleList)) and \
                       len(model.rna_encoder.encoder) > 0 and hasattr(model.rna_encoder.encoder[0], 'weight'):
                        l1_penalty = torch.sum(torch.abs(model.rna_encoder.encoder[0].weight)) * l1_lambda
                    else:
                        l1_penalty = torch.tensor(0.0, device=device) # No L1

                loss = rectified_flow.loss_fn(v_pred, target_velocity) + l1_penalty
            
            if use_amp:
                loss_scaler(loss, optimizer, model.parameters(), update_grad=True)
            else:
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
            
            train_loss_metric.update(loss.detach()) # Detach loss before metric update
            progress_bar.set_postfix(loss=loss.item())

        current_train_loss = train_loss_metric.compute().item()
        train_losses.append(current_train_loss)
        progress_bar.close()

        # Validation
        model.eval()
        val_loss_metric.reset()
        val_progress_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} - Validation", leave=False)
        with torch.no_grad():
            for batch in val_progress_bar:
                gene_expr = batch['gene_expr'].to(device)
                target_images = batch['image'].to(device)
                gene_mask = batch.get('gene_mask', None)
                if gene_mask is not None:
                    gene_mask = gene_mask.to(device)
                num_cells = None
                if is_multi_cell and 'num_cells' in batch:
                    num_cells = batch['num_cells']

                t = torch.rand(gene_expr.shape[0], device=device)
                path_sample = rectified_flow.sample_path(x_1=target_images, t=t)
                x_t = path_sample["x_t"]
                target_velocity = path_sample["velocity"]
                
                with torch.amp.autocast(device_type=device.type, enabled=use_amp):
                    if is_multi_cell:
                        v_pred = model(x_t, t, gene_expr, num_cells, gene_mask)
                        if hasattr(model, 'rna_encoder') and hasattr(model.rna_encoder, 'cell_encoder') and \
                           isinstance(model.rna_encoder.cell_encoder, (nn.Sequential, nn.ModuleList)) and \
                           len(model.rna_encoder.cell_encoder) > 0 and hasattr(model.rna_encoder.cell_encoder[0], 'weight'):
                            l1_penalty = torch.sum(torch.abs(model.rna_encoder.cell_encoder[0].weight)) * l1_lambda
                        else:
                            l1_penalty = torch.tensor(0.0, device=device)
                    else:
                        v_pred = model(x_t, t, gene_expr, gene_mask)
                        if hasattr(model, 'rna_encoder') and hasattr(model.rna_encoder, 'encoder') and \
                           isinstance(model.rna_encoder.encoder, (nn.Sequential, nn.ModuleList)) and \
                           len(model.rna_encoder.encoder) > 0 and hasattr(model.rna_encoder.encoder[0], 'weight'):
                            l1_penalty = torch.sum(torch.abs(model.rna_encoder.encoder[0].weight)) * l1_lambda
                        else:
                            l1_penalty = torch.tensor(0.0, device=device)
                            
                    loss = rectified_flow.loss_fn(v_pred, target_velocity) + l1_penalty
                
                val_loss_metric.update(loss.detach())
                val_progress_bar.set_postfix(loss=loss.item())
        
        current_val_loss = val_loss_metric.compute().item()
        val_losses.append(current_val_loss)
        val_progress_bar.close()
        
        lr_scheduler.step()
        
        logger.info(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {current_train_loss:.4f}, Val Loss: {current_val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.2e}")
        
        # Save latest checkpoint
        checkpoint_data = {
            'epoch': epoch, # Epoch just completed
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': lr_scheduler.state_dict(),
            'scaler_state_dict': loss_scaler.state_dict() if use_amp else None,
            'train_losses': train_losses,
            'val_losses': val_losses,
            'best_val_loss': best_val_loss,
            'epochs_no_improve': epochs_no_improve,
            'torch_rng_state': torch.get_rng_state(),
            'np_rng_state': np.random.get_state(),
            # 'args': vars(args) # Optionally save args for full reproducibility
        }
        torch.save(checkpoint_data, latest_checkpoint_path)
        logger.debug(f"Saved latest checkpoint to {latest_checkpoint_path} at end of epoch {epoch}")

        if current_val_loss < best_val_loss:
            best_val_loss = current_val_loss
            epochs_no_improve = 0
            # Save best model (original script format)
            torch.save({
                "model": model.state_dict(),
                "epoch": epoch,
                "val_loss": current_val_loss
                # Add other relevant args if needed, e.g., model config
            }, best_model_path)
            logger.info(f"New best model saved to {best_model_path} with validation loss: {best_val_loss:.4f}")
        else:
            epochs_no_improve += 1
            logger.info(f"Validation loss did not improve. EarlyStopping counter: {epochs_no_improve}/{patience}")
            if epochs_no_improve >= patience:
                logger.info(f"Early stopping triggered after {epoch+1} epochs.")
                break
    
    logger.info("Training finished.")
    return train_losses, val_losses

# generate_images_with_rectified_flow seems okay as is for preemption,
# as it's an inference function and doesn't maintain state across calls in a way
# that preemption would typically disrupt its internal logic for a single generation run.
# The model it uses will be loaded from a checkpoint.

def generate_images_with_rectified_flow(
    model,
    rectified_flow,
    gene_expr,
    device,
    num_steps=100,
    gene_mask=None,
    num_cells=None, # For multi-cell
    is_multi_cell=False
):
    """
    Generate cell images from gene expression profiles using rectified flow and DOPRI5 solver.
    (Unchanged from original, as it's for inference)
    """
    model.eval() # Ensure model is in eval mode

    # Create the solver with a wrapper to handle model's specific forward signature
    class ModelWrapper:
        def __init__(self, model_to_wrap, is_mc, mc_num_cells, mc_gene_mask):
            self.model_to_wrap = model_to_wrap
            self.is_mc = is_mc
            self.mc_num_cells = mc_num_cells # Expected to be a tensor or list of counts for the batch
            self.mc_gene_mask = mc_gene_mask # Gene mask for multi-cell

            # Inherit properties for solver
            self.img_channels = model_to_wrap.img_channels
            self.img_size = model_to_wrap.img_size

        def __call__(self, x, t, rna_expr_for_solver):
            # rna_expr_for_solver is gene_expr passed to solver.generate_sample
            if self.is_mc:
                return self.model_to_wrap(x, t, rna_expr_for_solver, self.mc_num_cells, self.mc_gene_mask)
            else: # Single-cell
                # gene_mask for single-cell is passed as mc_gene_mask here (if is_mc is False)
                return self.model_to_wrap(x, t, rna_expr_for_solver, self.mc_gene_mask)

    # Pass the correct gene_mask and num_cells to the wrapper based on model type
    current_gene_mask = gene_mask # This will be used by the wrapper
    current_num_cells = num_cells if is_multi_cell else None

    model_wrapper = ModelWrapper(model, is_multi_cell, current_num_cells, current_gene_mask)
    solver = DOPRI5Solver(model_wrapper, rectified_flow)

    with torch.no_grad():
        generated_images = solver.generate_sample(
            rna_expr=gene_expr, # This is what solver's __call__ will receive as rna_expr_for_solver
            num_steps=num_steps,
            device=device
        )

    generated_images = torch.clamp(generated_images, 0, 1) # Assuming images are [0,1]
    return generated_images